\section{Method}
\subsection{Model Structure}
Let's now have a look at the model implemented in the paper. The model is based on the MobileNet-v2 \cite{DBLP:journals/corr/abs-1801-04381}, which is an architecture specifically designed for mobile devices (low computational power). The MobileNetV2 design employs an inverted residual structure, which differs from traditional residual models by utilizing slender bottleneck layers for both input and output within the residual block. Instead of expanded representations in the input, MobileNetV2 employs lightweight depthwise convolutions to filter features in the intermediate expansion layer. 

The structure is U-shape with a x32 downsampling in the encoder phase that combined with the residual architecture extracts global and spatial information about the image.


\subsection{Auxiliary Losses}
To make up for the lower complexity of the model (since it needs to run in real-time) two auxiliary losses are introduced.

\subsubsection{Boundary Loss}
Given that, portrait segmentation needs sharper edges and that the edge represents less than 10\% of the labeled dataset we use the focal loss to balance this fact. Focal loss $L_e$ is combined with cross-entropy loss $L_m$ in the following way:

\begin{equation}
    L=L_m + \lambda L_e
\end{equation}

\begin{equation}
    L_m = - \sum_{i=1}^{n}(y_ilog(p_i)+(1-y_i)log(1-p_i))
\end{equation}

\begin{equation}
    L_e = - \sum_{i=1}^{n}(1-p_i)^\gamma y_ilog(p_i)) + p_i^{\gamma}(1-y_i)log(1-p_i))
\end{equation}

Where $\gamma$ is the weight of the boundary loss, and the probability $p_i$ is computed as 

\begin{equation}
    p_i(z_j)=\frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}
\end{equation}


\subsubsection{Consistency Constraint Loss}
In general, in a segmentation process, hard labels (binary labels) are used. Given our complexity requirements, since it has been shown that for small models soft labels help in the training process, a new method to generate soft labels is proposed (usually generating soft labels requires a teacher model that needs extensive training). 
Given an input $A$ a second input $A'$ is generated simply by a texture enhancement transformation (change in brightness, contrast, sharpness, noise...).
At this point, both images are passed through the model obtaining the output $B$ and $B'$. $B'$ is worse than $B$ because it was generated by $A'$, a lower quality version of $A$. In the end, $B$ is used as a higher quality soft label for $B'$ and the KL-divergence is used to compute the loss between the two:

\begin{equation}
    L=L_M'+\alpha\times L_c 
\end{equation}

\begin{equation}
    L_c = \frac{1}{n}\sum_{i=1}^n q_i \times log\frac{q_i}{q_i'} \times T^2
\end{equation}

\begin{align}
\begin{split}
    L_m' = &- \sum_{i=1}^{n}(y_ilog(p_i)+(1-y_i)log(1-p_i)) \\ 
    &- \sum_{i=1}^{n}(y_ilog(p_i')+(1-y_i)log(1-p_i'))
\end{split}
\end{align}

Where $\alpha$ is used to balance the two losses, $T$ to smooth the output, $p_i$ and $p_i'$ are:

\begin{equation}
    p_i(z_j)=\frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}
    \qquad
    p_i'(z_j')=\frac{e^{z_j'}}{\sum_{k=1}^Ke^{z_k'}}
\end{equation}

and $q_i$ and $q_i'$ are:

\begin{equation}
    p_i(z_j)=\frac{e^{\frac{z_j}{T}}}{\sum_{k=1}^Ke^{\frac{z_j'}{T}}}
    \qquad
    p_i'(z_j')=\frac{e^{\frac{z_j'}{T}}}{\sum_{k=1}^Ke^{\frac{z_k'}{T}}}
\end{equation}

Adding this loss results in higher accuracy and robustness under different lighting conditions.